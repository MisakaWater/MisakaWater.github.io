{"pages":[],"posts":[{"title":"EFCore数据库反向工程","text":"之前的大数据比赛取消了，就去弄了另外一个项目。 程序是师兄写的，但是后台数据库不见了，只能自己生成一个数据库来测试，有一个特点是程序运行时会自动生成空的数据表，这样就不用很麻烦的对着java的代码复现数据表结构（ef真香），在写爬虫找数据的时候就想着，能不能用程序把这些表变成实体类呢，网上找了一番，找到了解决方法就重新整理一下记录下了。 源链接:https://www.cnblogs.com/qidakang/p/11302327.html 打开VS，新建一个工程，然后安装如下的包 反向MySQL数据库就安装: 1234MySql.Data.EntityFrameworkCorePomelo.EntityFrameworkCore.MySqlMicrosoft.EntityFrameworkCore.ToolsMicrosoft.VisualStudio.Web.CodeGeneration.Design 反向SqlServer数据库就安装: 1234Microsoft.EntityFrameworkCoreMicrosoft.EntityFrameworkCore.SqlServerMicrosoft.EntityFrameworkCore.ToolsMicrosoft.VisualStudio.Web.CodeGeneration.Design 安装好后打开[程序包控制台]，接着输入 MySQL版本： 1234第一次生成实体类：Scaffold-DbContext &quot;Server=127.0.0.1;port=3306;Database=db; User=root;Password=root;&quot;Pomelo.EntityFrameworkCore.MySql -OutputDir Models更新实体类：Scaffold-DbContext &quot;Server=127.0.0.1;port=3306;Database=db; User=root;Password=root;&quot; Pomelo.EntityFrameworkCore.MySql -OutputDir Models -Force SqlServer版本： 1234第一次生成实体：Scaffold-DbContext &quot;Server=127.0.0.1;port=3306;Database=db; User=root;Password=root;&quot;Microsoft.EntityFrameworkCore.SqlServer -OutputDir Models更新实体类：Scaffold-DbContext &quot;Server=127.0.0.1;port=3306;Database=db; User=root;Password=root;&quot;Microsoft.EntityFrameworkCore.SqlServer -OutputDir Models -Force 运行后会多出一个Models文件夹，里面就是这个数据库的所有表","link":"/2020/04/03/EFCore%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%8D%E5%90%91%E5%B7%A5%E7%A8%8B/"},{"title":"Hbase Shell 常用操作","text":"help1help 'create' hbase shell 操作 HBase是没有schema的，就是在创建表的时候不需要指定表中有哪些列，只需要指定有多少个列蔟 12# 创建订单表，表名为ORDER_INFO，该表有一个列蔟为C1create &quot;ORDER_INFO&quot;,&quot;C1&quot; 12# 查看表list 12345# 删除表# 1.disable &quot;TableName&quot;# 2.drop &quot;TableName&quot;disable &quot;ORDER_INFO&quot;drop &quot;ORDER_INFO&quot; 增删改查1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# 添加一条数据# put '表名','ROWKEY','列蔟名:列名','值'# 数据示例# ID STATUS PAY_MONEY PAYWAY USER_ID OPERATION_DATE CATEGORY# 000001 已提交 4070 1 4944191 2020-04-25 手机put &quot;ORDER_INFO&quot;, &quot;000001&quot;, &quot;C1:STATUS&quot;, &quot;已提交&quot;put &quot;ORDER_INFO&quot;, &quot;000001&quot;, &quot;C1:PAY_MONEY&quot;, 4070put &quot;ORDER_INFO&quot;, &quot;000001&quot;, &quot;C1:PAYWAY&quot;, 1put &quot;ORDER_INFO&quot;, &quot;000001&quot;, &quot;C1:USER_ID&quot;, &quot;4944191&quot;put &quot;ORDER_INFO&quot;, &quot;000001&quot;, &quot;C1:OPERATION_DATE&quot;, &quot;2020-04-25&quot;put &quot;ORDER_INFO&quot;, &quot;000001&quot;, &quot;C1:CATEGORY&quot;, &quot;手机&quot;# 查询数据# get 查询一行数据# get '表名',‘ROWKEY’get 'ORDER_INFO','000001'# 正确显示中文get &quot;ORDER_INFO&quot;, &quot;000001&quot;, {FORMATTER =&gt; 'toString'}# 显示多版本get 'cx_table_stu01','20200001',{COLUMNS=&gt;'cf1',VERSIONS=&gt;5}# 如果只能显示一个版本，查看表结构的version是不是为1desc 'cx_table_stu01'# 除了列（COLUMNS）修饰词外HBase还支持# Limit（限制查询结果行数）# STARTROW （ROWKEY起始行。会先根据这个key定位到region，再向后扫描）# STOPROW(结束行)# TIMERANGE（限定时间戳范围）# VERSIONS（版本数）# FILTER（按条件过滤行）# 更新数据# put '表名','ROWKEY','列蔟名:列名','更改值'# 将订单ID为000001的状态，更改为「已付款」put &quot;ORDER_INFO&quot;,&quot;000001&quot;, &quot;C1:STATUS&quot;, &quot;已付款&quot;# 删除数据# 删除列# 如果有多个版本只会删除当前版本# delete &quot;表名&quot;, &quot;ROWKEY&quot;, &quot;列蔟名:列名&quot;# 将订单ID为000001的状态列删除。delete &quot;ORDER_INFO&quot;, &quot;000001&quot;, &quot;C1:STATUS&quot;# 删除行# deleteall '表名','ROWKEY'# 将订单ID为000001的信息全部删除（删除所有的列）deleteall &quot;ORDER_INFO&quot;, &quot;000001&quot;# 增加/删除列族# 增加# alter '表名', '列族名'alter 'Student', 'teacherInfo'# 删除# alter '表名', {NAME =&gt; '列族名', METHOD =&gt; 'delete'}alter 'Student', {NAME =&gt; 'teacherInfo', METHOD =&gt; 'delete'}# 修改表# alter '表名',{name=&gt;&quot;列族&quot;,要修改的内容=&gt;xxx}alter 'PERSON3',{NAME=&gt;'cf1', VERSIONS=&gt;5} 删除数据的时候，其实 HBase 不是真的直接把数据删除掉，而是给某个列设置一个标志，然后查询数据的时候，有这个标志的数据，就不显示出来什么时候真正的删除数据呢？ 后台进程，专门来执行删除数据的操作 执行 delete 的时候 如果表中的某个列有对一个的几次修改，它会删除最近的一次修改 默认是保存 1 个保存的时间戳 有一个 version 属性 计数器数据量小/测试环境中计数123# 统计表行数# count &quot;表名&quot;count &quot;ORDER_INFO&quot; count 执行效率非常低，适用于百万级以下的小表 RowCount 统计 数据量大/生成环境中计数利用 hbase.RowCounter 包执行 MR 任务 启动yarn集群 启动mr-historyserver 12# 在 shell 中执行hbase org.apache.hadoop.hbase.mapreduce.RowCounter '表名' 扫描操作scan操作1234567891011121314151617181920# 全表扫描：scan &quot;表名&quot;（慎用，效率很低）# 查询订单所有数据scan &quot;ORDER_INFO&quot;, {FORMATTER =&gt; 'toString'}# 只查询列族”cf1”scan 'cx_table_stu01',{COLUMNS=&gt;'cf1'}# 只查询列族”cf1”下的”name”信息scan 'cx_table_stu01',{COLUMNS=&gt;'cf1:name'}# 限定只显示多少条: scan &quot;表名&quot;, {LIMIT =&gt; XXX}# 查询订单数据（只显示3条）scan &quot;ORDER_INFO&quot;, {FORMATTER =&gt; 'toString', LIMIT =&gt; 3}# 指定查询某几个列: scan &quot;表名&quot;, {LIMIT =&gt; XXX, COLUMNS =&gt; []}# 只查询订单状态以及支付方式，并且只展示3条数据scan &quot;ORDER_INFO&quot;, {FORMATTER =&gt; 'toString', LIMIT =&gt; 3, COLUMNS =&gt; ['C1:STATUS', 'C1:PAYWAY']}# 根据ROWKEY来查询：scan &quot;表名&quot;, {LIMIT =&gt; XXX, COLUMNS =&gt; [], ROWPREFIXFILTER =&gt; 'ROWKEY'}# 使用scan来根据rowkey查询数据，也是查询指定列的数据scan &quot;ORDER_INFO&quot;, {ROWPREFIXFILTER =&gt; '02602f66-adc7-40d4-8485-76b5632b5b53',FORMATTER =&gt; 'toString', LIMIT =&gt; 3, COLUMNS =&gt; ['C1:STATUS', 'C1:PAYWAY']} 过滤器 其实在hbase shell中，执行的ruby脚本，背后还是调用hbase提供的Java API 官方文档：https://hbase.apache.org/devapidocs/index.html 使用方法12345# 查看内置过滤器show_filters# 示例scan '表名',{Filter =&gt; &quot;过滤器(比较运算符, '比较器表达式')&quot;} 123# 使用 RowFilter 查询指定订单ID的数据# 查询订单的ID为：02602f66-adc7-40d4-8485-76b5632b5b53、订单状态以及支付方式scan &quot;ORDER_INFO&quot;, {FILTER =&gt; &quot;RowFilter(=,'binary:02602f66-adc7-40d4-8485-76b5632b5b53')&quot;, COLUMNS =&gt; ['C1:STATUS', 'C1:PAYWAY'], FORMATTER =&gt; 'toString'} 通过上图，可以分析得到，RowFilter过滤器接受两个参数， op——比较运算符 rowComparator——比较器 所以构建该Filter的时候，只需要传入两个参数即可 12345# 查询状态为「已付款」的订单scan &quot;ORDER_INFO&quot;,{FILTER =&gt; &quot;SingleColumnValueFilter('C1', 'STATUS', = , 'binary:已付款')&quot;,COLUMNS =&gt; ['C1:STATUS','C1:PAYWAY'],FORMATTER =&gt; 'toString'}# 查询支付方式为1，且金额大于3000的订单scan &quot;ORDER_INFO&quot;,{FILTER =&gt; &quot;SingleColumnValueFilter('C1', 'PAYWAY', = , 'binary:1') AND SingleColumnValueFilter('C1', 'PAY_MONEY', &gt; , 'binary:3000')&quot;,COLUMNS =&gt; ['C1:STATUS','C1:PAYWAY','C1:PAY_MONEY'],FORMATTER =&gt; 'toString'} HBase shell中比较默认都是字符串比较，所以如果是比较数值类型的，会出现不准确的情况 例如：在字符串比较中4000是比100000大的 rowkey 过滤器 RowFilter 实现行键字符串的比较和过滤 PrefixFilter rowkey前缀过滤器 KeyOnlyFilter 只对单元格的键进行过滤和显示，不显示值 FirstKeyOnlyFilter 只扫描显示相同键的第一个单元格，其键值对会显示出来 InclusiveStopFilter 替代 ENDROW 返回终止条件行 列过滤器 FamilyFilter 列簇过滤器 QualifierFilter 列标识过滤器，只显示对应列名的数据 ColumnPrefixFilter 对列名称的前缀进行过滤 MultipleColumnPrefixFilter 可以指定多个前缀对列名称过滤 ColumnRangeFilter 过滤列名称的范围 值过滤器 ValueFilter 值过滤器，找到符合值条件的键值对 SingleColumnValueFilter 在指定的列蔟和列中进行比较的值过滤器 SingleColumnValueExcludeFilter 排除匹配成功的值 其他过滤器 ColumnPaginationFilter 对一行的所有列分页，只返回 [offset,offset+limit] 范围内的列 PageFilter 对显示结果按行进行分页显示 TimestampsFilter 时间戳过滤，支持等值，可以设置多个时间戳 ColumnCountGetFilter 限制每个逻辑行返回键值对的个数，在 get 方法中使用 DependentColumnFilter 允许用户指定一个参考列或引用列来过滤其他列的过滤器 比较器 比较器 描述 BinaryComparator 匹配完整字节数组 BinaryPrefixComparator 匹配字节数组前缀 BitComparator 匹配比特位 NullComparator 匹配空值 RegexStringComparator 匹配正则表达式 SubstringComparator 匹配子字符串 比较器表达式 比较器 表达式语言缩写 BinaryComparator binary:值 BinaryPrefixComparator binaryprefix:值 BitComparator bit:值 NullComparator null RegexStringComparator regexstring:正则表达式 SubstringComparator substring:值 累加器（INCR）incr可以实现对某个单元格的值进行原子性计数。语法如下： incr ‘表名’,’rowkey’,’列蔟:列名’,累加值（默认累加1） 如果某一列要实现计数功能，必须要使用incr来创建对应的列 使用put创建的列是不能实现累加的 12345678910111213141516# 通过 scan/get 无法直接查看cnthbase(main):004:0&gt; scan &quot;NEWS_VISIT_CNT&quot;, {FORMATTER =&gt; 'toString', LIMIT =&gt; 3}ROW COLUMN+CELL 0000000001_00:00-01:00 column=C1:CNT, timestamp=2021-11-19T23:12:42.890, value= 0000000001_00:00-01:00 column=C1:TIME_RANGE, timestamp=2021-11-19T23:12:43.153, value=00:00-01:00 0000000002_01:00-02:00 column=C1:CNT, timestamp=2021-11-19T23:12:42.907, value= 0000000002_01:00-02:00 column=C1:TIME_RANGE, timestamp=2021-11-19T23:12:43.165, value=01:00-02:00 0000000003_02:00-03:00 column=C1:CNT, timestamp=2021-11-19T23:12:42.912, value={ 0000000003_02:00-03:00 column=C1:TIME_RANGE, timestamp=2021-11-19T23:12:43.171, value=02:00-03:003 row(s)Took 0.1625 seconds# 需要使用 get_counter# get_counter '表','ROWKEY',&quot;列蔟名:列名&quot;get_counter 'NEWS_VISIT_CNT','0000000020_01:00-02:00','C1:CNT' 123# 对0000000020新闻01:00 - 02:00访问计数+1# incr '表','ROWKEY',&quot;列蔟名:列名&quot;incr 'NEWS_VISIT_CNT','0000000020_01:00-02:00','C1:CNT' 实用命令1234567891011121314151617181920212223# 显示服务器状态status# 显示HBase当前用户whoami# 查看表结构describe 'TableName'# 查看表是否存在exists 'TableName'# 查看表是否禁用/启用is_enabled 'TableName'is_disabled 'TableName'# 新增列蔟alter 'TableName', 'C3'# 删除列蔟alter 'TableName', 'delete' =&gt; 'C3'# 清空表数据truncate 'TableName' 执行 hbase shell 脚本12# hbase shell 'script file path'hbase shell ~/ORDER_INFO.txt tipsweb默认端口16010","link":"/2022/01/23/Hbase-Shell-%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"},{"title":"JS 实现 URL 变换","text":"前言今天无聊找东西玩翻书签翻到了 http://glench.com/hash/就想着找源码搬到 blog 上，转了半天发现作者没开源，就有了下文。 演示警告！请使用无痕模式再点击下面的按钮，不然会有一堆的历史记录。表情循环：click滚动条位置动画：click 代码 ver0.2 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131//修改 url 的方法//sub: 数组下标//arr: 表情数组async function setUrl(text) { if (!!(window.history &amp;&amp; history.pushState)) { history.replaceState(null, null, window.location.href.split('#')[0] = text); }}//修改 url 为默认 urlasync function cleanUrl(url) { if (!!(window.history &amp;&amp; history.pushState)) { history.replaceState(null, null, window.location.href.split('#')[0] = url); }}function getPageHeight() { let body = document.body, html = document.documentElement; let height = Math.max(body.scrollHeight, body.offsetHeight, html.clientHeight, html.scrollHeight, html.offsetHeight); return height;}function getWindowHeight() { let height = window.innerHeight; return height;}//循环动画使用的表情var kaomoji = [ &quot;(｀･ω･)&quot;, &quot;(〃∀〃)&quot;, &quot;w(ﾟДﾟ)w&quot;, &quot;_(:з」∠)_&quot;]var emoji = [ &quot;🌶️&quot;, &quot;💉&quot;, &quot;💦&quot;, &quot;🐂&quot;, &quot;🍺&quot;]var bar = [ &quot;-&quot;, &quot;0&quot;]var defUrl = window.location.href.split('#')[0]; //默认 urlvar scrollProgressIntervalId = null;function scrollBarClick() { UrlProgress(bar, 0, 1, 50, -1); let e = window.event; if (e != undefined) { obj = e.target || e.srcElement; } if (e.path[0].innerText == &quot;click&quot;) { e.path[0].innerText = &quot;stop&quot;; window.addEventListener('scroll', e =&gt; scrollProgress(bar), true); } else { alert(&quot;需要停止请刷新！&quot;); e.path[0].innerText = &quot;click&quot;; window.removeEventListener('scroll', scrollProgress(bar), true); }}function scrollProgress(arr) { let now; let scrollAvail = getPageHeight() - getWindowHeight(); // 可滚动的高度 let scrollTop = document.documentElement.scrollTop || document.body.scrollTop || window.pageYOffset; let length = 50; now = (scrollTop / scrollAvail) * length; now = parseInt(now); UrlProgress(arr, 0, 1, 50, now);}//arr 对应前景，背景数组//subBack 进度条背景//subFore 进度条前景//length 进度条总长//now 当前进度function UrlProgress(arr, subBack, subFore, length, now) { let args = &quot;&quot;; for (let i = 0; i &lt; length; i++) { args = args + arr[subBack] } if (now &gt;= 0) { args = args.split(''); args.splice(now, 1, arr[subFore]); args = args.join(''); } setUrl(args);}//循环动画var UrlLoopIntervalId = null;function UrlLoop(speed, arr, defUrl) { var e = window.event; if (e != undefined) { obj = e.target || e.srcElement; e.path[0].innerText = &quot;stop&quot;; } //上面： //选择触发方法的元素 if (UrlLoopIntervalId == null) { this.speed = speed || '1000'; var i = 0; UrlLoopIntervalId = setInterval(function () { setUrl(arr[i]); i++; if (i == arr.length) { i = 0; } }, speed); } else { clearInterval(UrlLoopIntervalId); cleanUrl(defUrl); UrlLoopIntervalId = null; e.path[0].innerText = &quot;click&quot;; }} ver0.1 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115//修改 url 的方法//sub: 数组下标//arr: 表情数组async function setUrl(text) { if (!!(window.history &amp;&amp; history.pushState)) { history.replaceState(null, null, window.location.href.split('#')[0] = text); }}//修改 url 为默认 urlasync function cleanUrl(url) { if (!!(window.history &amp;&amp; history.pushState)) { history.replaceState(null, null, window.location.href.split('#')[0] = url); }}function getPageHeight() { let body = document.body, html = document.documentElement; let height = Math.max(body.scrollHeight, body.offsetHeight, html.clientHeight, html.scrollHeight, html.offsetHeight); return height;}function getWindowHeight() { let height = window.innerHeight; return height;}//循环动画使用的表情var kaomoji = [ &quot;(｀･ω･)&quot;, &quot;(〃∀〃)&quot;, &quot;w(ﾟДﾟ)w&quot;, &quot;_(:з」∠)_&quot;]var emoji = [ &quot;🌶️&quot;, &quot;💉&quot;, &quot;💦&quot;, &quot;🐂&quot;, &quot;🍺&quot;]var bar = [ &quot;-&quot;, &quot;0&quot;]var defUrl = window.location.href.split('#')[0]; //默认 urlvar scrollProgressIntervalId = null;window.addEventListener('scroll', e =&gt; scrollProgress(bar), true)function scrollProgress(arr) { let now; let scrollAvail = getPageHeight() - getWindowHeight(); // 可滚动的高度 let scrollTop = document.documentElement.scrollTop || document.body.scrollTop || window.pageYOffset; let length = 50; now = (scrollTop / scrollAvail) * length; now = parseInt(now); UrlProgress(arr, 0, 1, 50, now);}//arr 对应前景，背景数组//subBack 进度条背景//subFore 进度条前景//length 进度条总长//now 当前进度function UrlProgress(arr, subBack, subFore, length, now) { let args = &quot;&quot;; for (let i = 0; i &lt; length; i++) { args = args + arr[subBack] } if (now &gt;= 0) { args = args.split(''); args.splice(now, 1, arr[subFore]); args = args.join(''); } setUrl(args);}//循环动画var UrlLoopIntervalId = null;function UrlLoop(speed, arr, defUrl) { var e = window.event; if (e != undefined) { obj = e.target || e.srcElement; e.path[0].innerText = &quot;stop&quot;; } //上面： //选择触发方法的元素 if (UrlLoopIntervalId == null) { this.speed = speed || '1000'; var i = 0; UrlLoopIntervalId = setInterval(function () { setUrl(arr[i]); i++; if (i == arr.length) { i = 0; } }, speed); } else { clearInterval(UrlLoopIntervalId); cleanUrl(defUrl); UrlLoopIntervalId = null; e.path[0].innerText = &quot;click&quot;; }} TODO 加一个进度条动画获取滚动位置改变进度。 同时打开多个弹窗，实现播放动画。 按钮解绑滚动条事件 在进度条的末端加进度的百分比","link":"/2019/07/21/JS-%E5%AE%9E%E7%8E%B0-URL-%E5%8F%98%E6%8D%A2/"},{"title":"Hive 练习题解答","text":"题目在 Github 上面找了一些题目来练习https://github.com/zzyb/Hive_interview-question 解答 answer 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236 -- 第一道面试题-- 编写Hive的HQL语句求出连续三天登陆的用户idselect distinct user_idfrom ( select user_id, lag(login_date,1) over (partition by user_id order by login_date) prev, login_date now, lead(login_date,1) over (partition by user_id order by login_date) next from login_log) t where datediff(now,prev) = 1 and datediff(next,now)=1;-- 第一道面试题-- 编写Hive的HQL语句求出每 个店铺的当月销售额和累计到当月的总销售额select name,mon,monMoney,accMonMoneyfrom(select name,mon,sum(monmoney) over (partition by name,mon) monMoney, sum(monmoney) over (partition by name order by mon) accMonMoney, row_number() over (partition by name,mon order by mon) topfrom fangwen2)t where top=1;-- 第二道面试题-- 编写Hive的HQL语句求出每个用户截止到每月为止的最大单月访问次数和累计到该月的总访问次数select usr,mon,num,col1 monPv,col2 maxMonPv,col3 accMonPv from(select usr,mon,num,sum(num) over (partition by usr,mon) col1, max(num) over (partition by usr,mon) col2, sum(num) over (partition by usr order by mon) col3, row_number() over (partition by usr,mon order by mon) col4 from fangwen) t where col4=1;select usr,mon,maxPv,totalPv from( select usr,mon,num, max(num) over (partition by usr,mon) maxPV, sum(num) over (partition by usr order by mon) totalPv, row_number() over (partition by usr,mon order by mon) top from fangwen) t where top=1;-- 第三道面试题-- 编写Hive的HQL语句按照day和mac分组，求出每组的销量累计总和，追加到每条记录的后面select day,mac,color,num, sum(num) over (partition by day,mac)from mac;-- TopN-- 第六道面试题-- 1、求出每一年的最高温度（年份，最高温度）select t.c_year,max(t.c_temp) from (select year(from_unixtime(unix_timestamp(substr(line,0,8),'yyyyMMdd'))) c_year,substr(line,9,2) c_temp from wendu) t group by t.c_year;select dt,year(dt),tempfrom( select concat_ws('-',substring(line,0,4),substring(line,5,2),substring(line,7,2)) dt, substring(line,9,2) temp, row_number() over (partition by substring(line,0,4) order by substring(line,9,2) DESC) top from wendu) subT where top=1;-- 2、求出每一年的最高温度是那一天（日期， 最高温度）select dt,tempfrom( select concat_ws('-',substring(line,0,4),substring(line,5,2),substring(line,7,2)) dt, substring(line,9,2) temp, row_number() over (partition by substring(line,0,4) order by substring(line,9,2) DESC) top from wendu) subT where top=1;select t.c_year,t.c_date,t.c_max from(select year(from_unixtime(unix_timestamp(substr(line,0,8),'yyyyMMdd'))) c_year,from_unixtime(unix_timestamp(substr(line,0,8),'yyyyMMdd')) c_date,row_number() over(partition by year(from_unixtime(unix_timestamp(substr(line,0,8),'yyyyMMdd'))) order by substr(line,9,2) DESC) as c_rank,substr(line,9,2) c_maxfrom wendu) t where t.c_rank = 1;-- 第四道面试题-- 求出所有数学课程成绩 大于 语文课程成绩的学生的学号-- 方法1 case whenselect * from(select sid, max(case course when 'yuwen' then course end) yuwen, max(case course when 'yuwen' then score end) yuwenScore, max(case course when 'shuxue' then course end) shuxue, max(case course when 'shuxue' then score end) shuxueScore, max(case course when 'yingyu' then course end) yingyu, max(case course when 'yingyu' then score end) yingyuScorefrom chengji group by sid) t where t.shuxueScore &gt; t.yuwenScore;-- 方法2 joinselect sid,t1score yuwen,t2score shuxue from(select * from ( select sid,course,score t1score from chengji where course = 'yuwen' ) t1join ( select sid,course,score t2score from chengji where course = 'shuxue') t2 on t1.sid = t2.sid) t3 where t3.t2score &gt; t3.t1score;-- 第五道面试题-- id_course.id id_course.course-- 1 a-- 1 b-- 1 c-- 1 e-- 2 a-- 2 c-- 2 d-- 2 f-- 3 a-- 3 b-- 3 c-- 3 e-- 转换为-- id a b b c d e f-- 1 1 1 1 1 0 1 0-- 2 1 0 0 1 1 0 1-- 3 1 1 1 1 0 1 0select id,max(case course when 'a' then 1 ELSE 0 end) a,max(case course when 'b' then 1 ELSE 0 end) b,max(case course when 'b' then 1 ELSE 0 end) b,max(case course when 'c' then 1 ELSE 0 end) c,max(case course when 'd' then 1 ELSE 0 end) d,max(case course when 'e' then 1 ELSE 0 end) e,max(case course when 'f' then 1 ELSE 0 end) ffrom id_course group by id;-- 第七道面试题-- 1,huangbo,45,a-c-d-f-- 2,xuzheng,36,b-c-d-e-- 3,huanglei,41,c-d-e-- 4,liushishi,22,a-d-e-- 5,liudehua,39,e-f-d-- 6,liuyifei,35,a-d-e-- 求出每种爱好中，年龄最大的人select id,name,maxAge,favorfrom ( select id, name, max(age) over (partition by favor) maxAge, favor, row_number() over (partition by favor order by age DESC) top from aihao lateral VIEW explode(split(favors, '-')) t1 as favor )t2 where top=1;-- 列出每个爱好年龄最大的两个人，并且列出名字。select id,name,maxAge,favorfrom ( select id, name, max(age) over (partition by favor) maxAge, favor, row_number() over (partition by favor order by age DESC) top from aihao lateral VIEW explode(split(favors, '-')) t1 as favor)t2 where top&lt;=2;-- 第十道面试题-- 日期，产品id，产品当日收入，产品当日成本-- 2018-03-01,a,3000,2500-- 2018-03-01,b,4000,3200-- 2018-03-01,c,3200,2400-- 2018-03-01,d,3000,2500-- 2018-03-02,a,3000,2500-- 2018-03-02,b,1500,800-- 2018-03-02,c,2600,1800-- 2018-03-02,d,2400,1000-- 2018-03-03,a,3100,2400-- 2018-03-03,b,2500,2100-- 2018-03-03,c,4000,1200-- 2018-03-03,d,2500,1900-- 2018-03-04,a,2800,2400-- 2018-03-04,b,3200,2700-- 2018-03-04,c,2900,2200-- 2018-03-04,d,2700,2500-- 2018-03-05,a,2700,1000-- 2018-03-05,b,1800,200-- 2018-03-05,c,5600,2200-- 2018-03-05,d,1200,1000-- 2018-03-06,a,2900,2500-- 2018-03-06,b,4500,2500-- 2018-03-06,c,6700,2300-- 2018-03-06,d,7500,5000-- 2018-04-01,a,3000,2500-- 2018-04-01,b,4000,3200-- 2018-04-01,c,3200,2400-- 2018-04-01,d,3000,2500-- 2018-04-02,a,3000,2500-- 2018-04-02,b,1500,800-- 2018-04-02,c,4600,1800-- 2018-04-02,d,2400,1000-- 2018-04-03,a,6100,2400-- 2018-04-03,b,4500,2100-- 2018-04-03,c,6000,1200-- 2018-04-03,d,3500,1900-- 2018-04-04,a,2800,2400-- 2018-04-04,b,3200,2700-- 2018-04-04,c,2900,2200-- 2018-04-04,d,2700,2500-- 2018-04-05,a,4700,1000-- 2018-04-05,b,3800,200-- 2018-04-05,c,5600,2200-- 2018-04-05,d,5200,1000-- 2018-04-06,a,2900,2500-- 2018-04-06,b,4500,2500-- 2018-04-06,c,6700,2300-- 2018-04-06,d,7500,5000-- 源数据3月份与4月份日利润相同，添加一条数据使其不同。insert into goods values ('2018-04-07','d','5000','3000');-- 1. 输出每个产品，在2018年期间，每个月的净利润，日均成本。select d_month,profit,avgCost from (select concat(year(dt),'-',month(dt)) d_month,name,income,cost, (sum(income) over(partition by concat(year(dt),'-',month(dt))) - sum(cost) over(partition by concat(year(dt),'-',month(dt)))) profit, avg(cost) over(partition by concat(year(dt),'-',month(dt))) avgCost, row_number() over (partition by concat(year(dt),'-',month(dt)) order by name) topfrom goods where year(dt) = '2018') t_month where t_month.top=1;-- 2. 输出每个产品，在2018年3月中每一天与上一天相比，成本的变化。select dt,cost, (cost - LAG(cost,1,cost) over(partition by concat(year(dt),'-',month(dt)) order by dt))from goods where concat(year(dt),'-',month(dt)) = '2018-3';-- 3. 输出2018年4月，有多少个产品总收入大于22000元，必须用一句SQL语句实现，且不允许使用关联表查询、子查询。select concat(year(dt),'-',month(dt)),name,sum(income) sumIncomefrom goods where concat(year(dt),'-',month(dt)) = '2018-4'group by name,concat(year(dt),'-',month(dt)) having sumIncome&gt;22000;-- 4. 输出2018年4月，总收入最高的那个产品，每日的收入，成本，过程使用over()函数。select g.dt,maxT.name,maxT.maxIncome,g.income,g.cost from( select name,totalIncome,max(totalIncome) over() maxIncome from( select dt,name,income,cost, sum(income) over (partition by name) totalIncome from goods where concat(year(dt),'-',month(dt)) = '2018-4') t limit 1) maxT join goods g on maxT.name=g.name where concat(year(dt),'-',month(dt)) = '2018-4';","link":"/2022/01/23/Hive-%E7%BB%83%E4%B9%A0%E9%A2%98%E8%A7%A3%E7%AD%94/"},{"title":"Mysql&#x2F;mariadb 主从数据库简单配置","text":"安装并启动数据库123yum install mariadb-server mariadb -ysystemctl enable mariadbsystemctl start mariadb master 配置1234567891011121314vi /etc/my.cnf# 添加配置，注意 id 不能重复# log-bin=mysql-bin# server-id=1 # 配置完成后重启数据库systemctl restart mariadb# 进入mysqlmysql -uroot# 添加slave用户GRANT REPLICATION SLAVE ON *.* to 'slave'@'%' identified by '000000';# 刷新权限FLUSH PRIVILEGES; slave 配置123456789101112131415161718192021vi /etc/my.cnf# 只需要添加server-id# server-id=2# 配置完成后重启数据库systemctl restart mariadb# 进入mysqlmysql# 添加主节点配置# log_file 和 log_pos 的值在主节点上使用 show master status; 查看change master to master_host='192.168.0.178',master_user='slave',master_password='000000',master_log_file='mysql-bin.000001' ,master_log_pos=463;# 启动 slave 节点start slave;# 查看主从状态show slave status\\G;# 如下显示则成功# Slave_IO_Running: Yes# Slave_SQL_Running: Yes 如果主从节点数据不同步需要手动同步12345# 全局锁库FLUSH TABLES WITH READ LOCK;# 解锁UNLOCK TABLES;","link":"/2022/01/23/Mysql-mariadb-%E4%B8%BB%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%80%E5%8D%95%E9%85%8D%E7%BD%AE/"},{"title":"Mysql 系数据库备份","text":"使用 mysqldump 备份123456# 备份全部数据库mysqldump -uroot --all-databases &gt; databases.sql# 备份指定数据库mysqldump -uroot databaseName &gt; databaseName.sql 导入 sql 数据123456# 导入mysql -uroot &lt; database.sql# 或者# 打开 mysql 客户端，使用 source 导入mysqlsource database.sql","link":"/2022/01/23/Mysql-%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD/"},{"title":"Windows10 下使用 IDEA 配置 Spark 的 Debug 环境","text":"之前报名了学校的一些大数据比赛，做的笔记整理一下发出来，供以后查看第一次用 ide 和 scala，不足之处还请批评指正 idea 的安装就不赘述了。 spark 在本地不需要安装。 jdk = 1.8 spark version = 2.0.0，比较老，但是是比赛的要求。 hadoop version = 2.6.0 scala version = 2.11.0 在 idea 中建立一个 maven 工程，注意路径不能包括中文 添加 pom.xml，示例在 Source\\Task3\\example\\pom.xml 中 pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;artifactId&gt;example&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;spark.version&gt;2.0.0&lt;/spark.version&gt; &lt;scala.version&gt;2.11&lt;/scala.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_${scala.version}&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_${scala.version}&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_${scala.version}&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_${scala.version}&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_${scala.version}&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.scala-tools&lt;/groupId&gt; &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt; &lt;version&gt;2.15.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.6.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.19&lt;/version&gt; &lt;configuration&gt; &lt;skip&gt;true&lt;/skip&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 在 Idea 左侧 Project 窗口中选中 src\\main 右键添加文件夹 scala 选中 scala 右键 make directory as -&gt; Sources Root scala 变成蓝色后右键 new -&gt; scala class -&gt;选择 object，输入名称 我们先写一个最简单的 CsvShow 程序让他能在本地运行并且 debug CsvShow Code 12345678910111213141516171819package org.example.spark.scalaimport org.apache.spark.sql.{DataFrame, SparkSession}import org.apache.spark.{SparkConf, SparkContext}object CsvShow { def main(args:Array[String]):Unit= { val spark: SparkSession = SparkSession.builder() .master(&quot;local[*]&quot;) .appName(&quot;CsvShow&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, &quot;file:/&quot;) .getOrCreate() val csv: DataFrame = spark.read.csv(&quot;../../../../data/mysql.csv&quot;) csv.show() val pdf: DataFrame = csv.toDF() pdf.show() spark.stop() }} 解压 Hadoop，在没有安装 hadoop 的情况下是不能进行 spark 程序的本地 debug 我们先下载如下两个文件 hadoop-2.6.0.tar.gz 和 hadooponwindows-master.zip 先将 hadoop 解压出来 再解压 hadooponwindows 到 hadoop 的根目录下，提示覆盖选择全覆盖 添加如下环境变量 变量名=HADOOP_HOME, 变量值=你解压的路径 在 path 中添加 %HADOOP_HOME%\\bin 变量 打开 cmd，输入 hadoop version，显示版本说明 hadoop 环境变量配置正确 配置 Hadoop 环境 修改 etc\\hadoop\\hadoop-env.cmd 中的 JAVA_HOME，如 JAVA_HOME=C:\\PROGRA~1\\Java\\jdk1.8.0_241，路径中不能有空格 在根目录下添加 datanode、namenode 和 tmp 文件夹 修改 etc\\hadoop\\hdfs-site.xml，添加如下 xml, 注意/D:/hadoop-2.6.0/namenode 和/D:/hadoop-2.6.0/datanode 这两个值需要时之前创建文件夹的路径 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/D:/hadoop-2.6.0/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/D:/hadoop-2.6.0/datanode&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 etc\\hadoop\\core-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 etc\\hadoop\\mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 etc\\hadoop\\yarn-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 以管理员身份打开 cmd 输入 hdfs namenode -format 来执行格式化 hdfs，执行后 namenode 文件里会自动生成一个 current 文件 cd 到根目录下 sbin 文件夹，输入 start-all.cmd 启动全部的 hadoop 服务 在相应的方法中右键 Debug “*”，就可以运行本地 debug 调试","link":"/2020/03/11/Windows10-%E4%B8%8B%E4%BD%BF%E7%94%A8-IDEA-%E9%85%8D%E7%BD%AE-Spark-%E7%9A%84-Debug-%E7%8E%AF%E5%A2%83/"},{"title":"Spark SQL 读写操作","text":"使用方法 spark-shell spark-sql scala code 数据源 CSV JSON Parquet ORC JDBC/ODBC connections Plain-text files 读取示例12345678910// 格式DataFrameReader.format(...).option(&quot;key&quot;, &quot;value&quot;).schema(...).load()// 示例spark.read.format(&quot;csv&quot;).option(&quot;mode&quot;, &quot;FAILFAST&quot;) // 读取模式.option(&quot;inferSchema&quot;, &quot;true&quot;) // 是否自动推断 schema.option(&quot;path&quot;, &quot;path/to/file(s)&quot;) // 文件路径.schema(someSchema) // 使用预定义的 schema .load() 读模式 描述 permissive 当遇到损坏的记录时，将其所有字段设置为 null，并将所有损坏的记录放在名为 _corruption t_record 的字符串列中 dropMalformed 删除格式不正确的行 failFast 遇到格式不正确的数据时立即失败 写入示例123456789// 格式DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()//示例dataframe.write.format(&quot;csv&quot;).option(&quot;mode&quot;, &quot;OVERWRITE&quot;) //写模式.option(&quot;dateFormat&quot;, &quot;yyyy-MM-dd&quot;) //日期格式.option(&quot;path&quot;, &quot;path/to/file(s)&quot;).save() Scala/Java 描述 SaveMode.ErrorIfExists 如果给定的路径已经存在文件，则抛出异常，这是写数据默认的模式 SaveMode.Append 数据以追加的方式写入 SaveMode.Overwrite 数据以覆盖的方式写入 SaveMode.Ignore 如果给定的路径已经存在文件，则不做任何操作 常用读写示例12345678910111213141516171819202122232425262728293031323334353637383940414243// 简写法spark.read.格式(&quot;路径&quot;)spark.read.json(&quot;/usr/file/json/emp.json&quot;)// 读取csvspark.read.format(&quot;csv&quot;).option(&quot;header&quot;, &quot;false&quot;) // 文件中的第一行是否为列的名称.option(&quot;mode&quot;, &quot;FAILFAST&quot;) // 是否快速失败.option(&quot;inferSchema&quot;, &quot;true&quot;) // 是否自动推断 schema.load(&quot;/usr/file/csv/dept.csv&quot;).show()// 写入csv,指定分隔符\\tdf.write.format(&quot;csv&quot;).mode(&quot;overwrite&quot;).option(&quot;sep&quot;, &quot;\\t&quot;).save(&quot;/tmp/csv/dept2&quot;)// json 读spark.read.format(&quot;json&quot;).option(&quot;mode&quot;, &quot;FAILFAST&quot;).load(&quot;/usr/file/json/dept.json&quot;).show(5)// json 写df.write.format(&quot;json&quot;).mode(&quot;overwrite&quot;).save(&quot;/tmp/spark/json/dept&quot;)// Parquet 读spark.read.format(&quot;parquet&quot;).load(&quot;/usr/file/parquet/dept.parquet&quot;).show(5)// Parquet 写df.write.format(&quot;parquet&quot;).mode(&quot;overwrite&quot;).save(&quot;/tmp/spark/parquet/dept&quot;)// 数据库读spark.read.format(&quot;jdbc&quot;).option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;) //驱动.option(&quot;url&quot;, &quot;jdbc:mysql://127.0.0.1:3306/mysql&quot;) //数据库地址.option(&quot;dbtable&quot;, &quot;help_keyword&quot;) //表名.option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;,&quot;root&quot;).load().show(10)// 数据库写val df = spark.read.format(&quot;json&quot;).load(&quot;/usr/file/json/emp.json&quot;)df.write.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:mysql://127.0.0.1:3306/mysql&quot;).option(&quot;user&quot;, &quot;root&quot;).option(&quot;password&quot;, &quot;root&quot;).option(&quot;dbtable&quot;, &quot;emp&quot;).save()","link":"/2022/01/23/Spark-SQL-%E8%AF%BB%E5%86%99%E6%93%8D%E4%BD%9C/"},{"title":"hive HQL 常用操作","text":"资料https://blog.csdn.net/u013411339/article/details/120795941 远程连接hivehttps://blog.csdn.net/qq_44065303/article/details/105820435 服务端要开启 hiveserver2 建表方式123456789101112-- 1.正常建表-- 建表后无数据create table tableName(col1 int,col2 int);-- 2.查询建表 as-- 建表后有数据，数据为查询结果-- 并不会带原表的分区（分区丢失），包扣一些字段的约束等create table tableName as select * from tableA;-- 3.结构建表 like-- 建表后无数据，表结构从 tableA 中复制create table tableName like tableA; 函数查询123456789101112-- 查看内置函数SHOW FUNCTIONS;-- 查看month相关的函数SHOW FUNCTIONS LIKE '*month*';-- 查看函数用法DESC FUNCTION function_name;-- 查看 add_months 函数的详细说明并举例DESC FUNCTION EXTENDED add_months; CTE1234-- 公用表表达式-- 创建一个临时结果集with tableName as (select * from tableB)select * from tableName; 时间转换函数1234567891011121314151617181920212223242526272829303132333435363738语法: unix_timestamp()返回值: bigint说明: 获得当前时区的UNIX时间戳hive&gt; select unix_timestamp() from tableName;1616906976语法: from_unixtime(bigint unixtime[, string format])返回值: string说明: 转化UNIX时间戳（从1970-01-01 00:00:00 UTC到指定时间的秒数）到当前时区的时间格式hive&gt; select from_unixtime(1616906976,'yyyyMMdd') from tableName;20210328语法: unix_timestamp(string date)返回值: bigint说明: 转换格式为&quot;yyyy-MM-dd HH:mm:ss&quot;的日期到UNIX时间戳。如果转化失败，则返回0。hive&gt; select unix_timestamp('2021-03-08 14:21:15') from tableName;1615184475语法: unix_timestamp(string date, string pattern)返回值: bigint说明: 转换pattern格式的日期到UNIX时间戳。如果转化失败，则返回0。hive&gt; select unix_timestamp('2021-03-08 14:21:15','yyyyMMdd HH:mm:ss') from tableName;1615184475语法: to_date(string timestamp)返回值: string说明: 返回日期时间字段中的日期部分。hive&gt; select to_date('2021-03-28 14:03:01') from tableName;2021-03-28语法: year(string date)返回值: int说明: 返回日期中的年。hive&gt; select year('2021-03-28 10:03:01') from tableName;2021hive&gt; select year('2021-03-28') from tableName;2021 日期函数1234567891011121314151617语法: datediff(string enddate, string startdate)返回值: int说明: 返回结束日期减去开始日期的天数。hive&gt; select datediff('2020-12-08','2012-05-09') from tableName;213语法: date_add(string startdate, int days)返回值: string说明: 返回开始日期startdate增加days天后的日期。hive&gt; select date_add('2020-12-08',10) from tableName;2020-12-18语法: date_sub (string startdate, int days)返回值: string说明: 返回开始日期startdate减少days天后的日期。hive&gt; select date_sub('2020-12-08',10) from tableName;2020-11-28 窗口函数窗口聚合函数sum12345-- 共4种用法sum(...) over() -- 相当于对全表进行求和sum(...) over(order by ...) -- 连续累积求和，排序，不分组sum(...) over(partition by ...) -- 同组内所行求和，只分组不排序sum(...) over(partition by ... order by ...) -- 在每个分组内，连续累积求和，分组+排序 partition by order by 窗口表达式窗口表达式提供了一种控制行范围的能力如往前，往后n行 可以写多个列 12345678910111213141516-- 默认从第一行到当前行sum(...) over(partition by ... order by ...) sum(...) over(partition by ... order by ... , ...) -- 按多列分区sum(...) over(partition by ... order by ... rows between unbounded preceding and current row) -- 向前3行至当前行sum(...) over(partition by ... order by ... rows between 3 preceding and current row) -- 向前3行,向后1行sum(...) over(partition by ... order by ... rows between 3 preceding and 1 following) -- 当前行到最后一行sum(...) over(partition by ... order by ... rows between current row and unbounded following) -- 第一行至最后一行，即分组内所有行sum(...) over(partition by ... order by ... rows between unbounded preceding and unbounded following) 窗口排序函数1234-- row_number家族，在每个分组中，为每行分配一个从1开始的唯一序号row_number -- 不考虑重复rank -- 考虑重复，挤占后续位置dense_rank -- 考虑重复，不挤占后续位置 1234567-- 窗口排序函数 ntile将每个分组内的数据分为指定的若干个桶内，并且为每一个桶分配一个桶编号select cookieid,createtime,pvntile(3) over(partition by cookieid order by createtime)from website_pv_infoorder by cookieid,createtime; 抽样函数随机抽样 random随机，但是速度慢 12select * from website_pv_info order by rand() limit 2;select * from website_pv_info distribute by rand() sort by rand() limit 2; block 抽样速度快，但是不随机 123456-- 按行数抽样select * from website_pv_info tablesample (1 rows);-- 根据数据大小百分比抽样select * from website_pv_info tablesample (50 percent);-- 根据数据大小抽样，支持b,k,m,gselect * from website_pv_info tablesample (1B); bucket table 基于分桶表抽样速度快+随机 tablesample(BUCKET x OUT OF y [ON colname]) 123-- 根据整行数据进行抽样select * from website_pv_info tablesample ( bucket 1 out of 2 on rand()); 类型转换123456789-- CAST可以转换的类型-- 1．隐式类型转换规则如下-- （1）任何整数类型都可以隐式地转换为一个范围更广的类型，如 TINYINT 可以转换成 INT，INT 可以转换成 BIGINT。-- （2）所有整数类型、FLOAT 和 STRING 类型都可以隐式地转换成 DOUBLE。-- （3）TINYINT、SMALLINT、INT 都可以转换为 FLOAT。-- （4）BOOLEAN 类型不可以转换为任何其它的类型。CAST('1' AS INT) -- 可以正常执行CAST('X' AS INT) -- 执行失败返回 NULL concat 函数12345678910111213141516-- CONCAT 函数用于将多个字符串连接成一个字符串。-- CONCAT(str1,str2,…) SELECT CONCAT(id,',',name) AS con FROM info LIMIT 1;返回结果为+----------+| con |+----------+| 1,BioCyc |+----------+-- CONCAT_WS() 指定参数之间的分隔符SELECT CONCAT_WS('_',id,name) AS con_ws FROM info LIMIT 1;+----------+| con_ws |+----------+| 1_BioCyc |+----------+ case when123456789-- 简单 caseCASE sexWHEN '1' THEN '男'WHEN '2' THEN '女'ELSE '其他' END-- 搜索 caseCASE WHEN sex = '1' THEN '男'WHEN sex = '2' THEN '女'ELSE '其他' END UDTF（explode）和侧视图1234567891011121314151617181920-- explode -- 接收 array 和 map 转换成多行select explode(`array`(1,2,3,4,5));-- Result:-- col-- 1-- 2-- 3-- 4-- 5select explode(`map`(&quot;name&quot;,&quot;xxx&quot;,'sex','男','age','13'));-- Result:-- key value-- name xxx-- sex 男-- age 13-- 侧视图结合 explode-- 侧视图语法select ... from tableA lateral view UDTF(xxx) 表别名 as 列别名1,列别名2,列别名3; 行列转换123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113-- 多行转多列-- 利用case when-- 原表-- students.id students.name students.subject students.score-- 1 小明 语文 87-- 2 张三 语文 27-- 3 王五 语文 69-- 4 李四 语文 99-- 5 小明 数学 86-- 6 马六 数学 33-- 7 李四 数学 44-- 8 小红 数学 50-- 转换后-- name chinese math-- 小明 87 86-- 小红 0 50-- 张三 27 0-- 李四 99 44-- 王五 69 0-- 马六 0 33select name,max(case subject when '语文' then score else 0 end) chinese,max(case subject when '数学' then score else 0 end) mathfrom students group by name;-- 多行转单列-- 转换前-- cookie.cookieid cookie.createtime cookie.pv-- cookie1 2015-04-11 5-- cookie1 2015-04-12 7-- cookie1 2015-04-10 1-- cookie1 2015-04-13 3-- cookie1 2015-04-14 2-- cookie1 2015-04-15 4-- cookie1 2015-04-16 4-- cookie2 2015-04-10 2-- cookie2 2015-04-11 3-- cookie2 2015-04-12 5-- cookie2 2015-04-13 6-- cookie2 2015-04-14 3-- cookie2 2015-04-15 9-- cookie2 2015-04-16 7-- 转换后-- cookieid createtimes pvs-- cookie1 [&quot;2015-04-10&quot;,&quot;2015-04-11&quot;,&quot;2015-04-12&quot;,&quot;2015-04-13&quot;,&quot;2015-04-14&quot;,&quot;2015-04-15&quot;,&quot;2015-04-16&quot;] [1,5,7,3,2,4,4]-- cookie2 [&quot;2015-04-10&quot;,&quot;2015-04-11&quot;,&quot;2015-04-12&quot;,&quot;2015-04-13&quot;,&quot;2015-04-14&quot;,&quot;2015-04-15&quot;,&quot;2015-04-16&quot;] [2,3,5,6,3,9,7]select cookieid,collect_list(createtime) createtimes,collect_list(pv) pvs from cookie group by cookieid;-- 其他函数COLLECT_LIST(col) -- 多行合并成一行，不去重COLLECT_SET(col) -- 多行合并成一行，将某字段的值进行去重汇总CONCAT([string|col],[string|col],[string|col],....) -- 返回输入字符串连接后的结果，支持任意个输入字符串CONCAT_WS('分隔符',[array|map]) -- 指定分隔符-- 多列转多行-- 主要使用 union 关键字-- 转换前-- rows2cols.col1 rows2cols.col2 rows2cols.col3 rows2cols.col4-- a 1 2 3-- b 4 5 6-- 转换后-- _u1.col1 _u1.col2 _u1.col3-- a c 1-- a d 1-- a e 1-- b c 4-- b d 4-- b e 4select col1,'c' as col2,col2 as col3 from rows2colsunion allselect col1,'d' as col2,col2 as col3 from rows2colsunion allselect col1,'e' as col2,col2 as col3 from rows2cols;-- 单列转多行-- 转换前-- aihao.id aihao.name aihao.age aihao.favors-- 1 huangbo 45 a-c-d-f-- 2 xuzheng 36 b-c-d-e-- 3 huanglei 41 c-d-e-- 4 liushishi 22 a-d-e-- 5 liudehua 39 e-f-d-- 6 liuyifei 35 a-d-e-- 转换后-- id name age favorslist-- 1 huangbo 45 a-- 1 huangbo 45 c-- 1 huangbo 45 d-- 1 huangbo 45 f-- 2 xuzheng 36 b-- 2 xuzheng 36 c-- 2 xuzheng 36 d-- 2 xuzheng 36 e-- 3 huanglei 41 c-- 3 huanglei 41 d-- 3 huanglei 41 e-- 4 liushishi 22 a-- 4 liushishi 22 d-- 4 liushishi 22 e-- 5 liudehua 39 e-- 5 liudehua 39 f-- 5 liudehua 39 d-- 6 liuyifei 35 a-- 6 liuyifei 35 d-- 6 liuyifei 35 eselect id,name,age,t.col1 as favorsList from aihao lateral VIEW explode(split(favors,'-')) t as col1;-- lateral VIEW + explode 常见案例查看前百分之N1234567891011/* NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型 ntile(n)和where sorter = m 构成 n/m，如：ntile（2）和where sorted = 1 构成显示所有列的1/2 20% = 1/5 ==&gt; ntile(5),where sorted = 1 */SELECT *FROM ( SELECT name, orderdate, cost, NTILE(5) OVER (ORDER BY orderdate DESC) sorted FROM business ) tWHERE sorted = 1; TOPN 问题123select * from ( select ROW_NUMBER() over(partition by ... order by ... DESC) as rank from tableName ) t where t.rank &lt; 3; 与前一天相比12345LAG(列名,前几行,'默认值')LAG(name, 1) LAG(name, 1,'default')-- 表示取前一条记录的name的值。-- LEAD()函数与此类似，不过它是查询某字段的后N条记录的值。 连续登录/连续问题 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758-- 原数据1,2020-01-011,2020-01-021,2020-01-071,2020-01-081,2020-01-091,2020-01-102,2020-01-012,2020-01-022,2020-01-032,2020-01-043,2020-01-023,2020-01-033,2020-01-04-- 两种方案-- 1.自连接，构建笛卡尔积-- 2. 窗口函数-- 自连接-- 两天的连续登录问题使用自连接比较好做，大于2天不适合用自连接-- 1.自连接select a.user_id a_id,a.login_date a_dt, b.user_id b_id,b.login_date b_dtfrom login_log a,login_log b;-- 2.查找用户id相同并且时间差为1的记录select *from ( select a.user_id a_id,a.login_date a_dt, b.user_id b_id,b.login_date b_dt from login_log a,login_log b) t where t.a_id=t.b_id and datediff(t.a_dt,t.b_dt)=1;-- 提取id，并去重select distinct t1.a_idfrom ( select * from ( select a.user_id a_id,a.login_date a_dt, b.user_id b_id,b.login_date b_dt from login_log a,login_log b ) t where t.a_id=t.b_id and datediff(t.a_dt,t.b_dt)=1) t1;-- 窗口函数-- 适合于连续N天的问题select distinct user_id from ( select user_id,login_date, date_add(login_date,4-1) as nextday, lead(login_date,4-1) over (partition by user_id order by login_date) as nextlogin from login_log) t where nextday = nextlogin;-- nextday，手动算出 N 天后的时间-- nextlogin，向下取 N 行进行验证 级联/累加求和1234567891011121314151617181920212223242526272829303132333435363738-- 编写Hive的HQL语句求出每个用户截止到每月为止的最大单月访问次数和累计到该月的总访问次数-- 原表usr,mon,numA,2015-01,5A,2015-01,15B,2015-01,5A,2015-01,8B,2015-01,25A,2015-01,5A,2015-02,4A,2015-02,6B,2015-02,10B,2015-02,5A,2015-03,16A,2015-03,22B,2015-03,23B,2015-03,10B,2015-03,11-- 两种方法-- 1.group by 加自连接-- 2.窗口函数-- 1.group by 加自连接-- 过于复杂，不写了-- https://www.bilibili.com/video/BV1L5411u7ae?p=126-- 方法2-- 窗口函数-- 关键点是sum(num) over (partition by usr order by mon) accPVwith t as ( select usr,mon,num, row_number() over (partition by usr,mon order by num DESC) top, sum(num) over (partition by usr order by mon) accPV from fangwen)select usr,mon,num,accPV from t where top=1;","link":"/2022/01/23/hive-HQL-%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"},{"title":"关于我一巴掌拍坏硬盘导致博客源代码丢失一年半后重建博客这件事","text":"前言在上古时期，记忆已经渐渐模糊。只记得是一个午后，遥远的世界树在被邪恶的夜魇入侵，不知道那场战斗持续了多久，不知道是为什么而战，甚至最后的胜利消息也是通过只言片语所得知。我作为天辉方的爱由莎与夜魇的战斗是那么的焦灼。因为一次次的操作失误，一次次的倒下，头顶上的怒气条慢慢的攒满，直到那一巴掌的落下，怒气条空了。一开始，以为这只是一点桌面图标的消失、一个小小的弹窗提示、一个普通的报错，直到一整块硬盘的消失。当我从远古遗迹中回到现实位面，重新按下笔记本电脑开机键，一阵如炒豆子般的声音，引来方圆十里人们的围观，舍友们以为我偷偷在床上炒菜不分给他们。网上流行过一种说法，人死前会回忆他的一生，这时我相信这块硬盘也在回忆它的一生。宛如看见硬盘文件如到持续时间的幻象一样，消失得无影无踪。 太长不看：打游戏上头一巴掌拍坏机械硬盘。 步骤在一年半之后终于有空（其实是懒）根据 github page 上面的发布内容重新做一个 hexo 的源码文件夹。 这里有个问题，重建 blog 需要几步呢？ A. 3 B. 5 C. 钝角 1. 重装一个 hexo因为损坏的硬盘是 D 盘，hexo 是安装在 C 盘并不受影响，但是版本落后不少，在这一年半期间 npm 也升级过，在使用旧版 hexo 和 npm 碰到了兼容性问题，就直接重装 npm 和 hexo，也相当于进行一次升级。 直接在 nodejs 下载安装程序覆盖安装即可 2. 调整和优化一些设置旧 blog 主题用的 icarus 也没找到其他好看的主题就继续沿用。 但是旧 blog 的 icarus 是自定义过的，就需要对着旧 blog 的样式调整一下。 3. 复制内容将旧 blog 的 HTML 内容复制为 Markdown 格式。 这里使用的工具是 bejson 家的，挺好用。 但是也会遇到一些问题，如标题转换异常，还有通过 hexo 的高亮代码 codeblock 包裹的也会有这种转换问题，这就需要重新写过了。 标题转换，感觉应该算特性，在 typora 里面看的话会不正常，在 hexo 生成的内容则是显示正常的。 有序列表的序号也会有错误，需要手动修改 图片题注格式不正常（不是默认格式），这里是因为转换后加上了 html 的标签，md 解析后格式不是默认格式，去掉 html 标签即可 4. 自定义 JS 的引入这里需要引入自定义 JS 是因为第一篇内容里面写的代码需要。 也很简单吧 js 文件放到 themes\\主题名称\\source\\js 就好，然后在 markdown 里面引用就行。 有一个小细节要注意的是，路径要以 / 开始，写绝对路径，相对路径如果打开到内容页的话会无法加载 js 文件。 5. 部署到 github page首先安装一下 deployer 1npm install hexo-deployer-git --save 然后添加一下 _config.yml 配置 1234deploy: type: git repo: https://github.com/MisakaWater/MisakaWater.github.io branch: master 再运行一下 123hexo cleanhexo ghexo d 就开始推送了 其实整个过程难度不大，就是想记一下这个硬盘的故事，太逗了","link":"/2022/01/22/%E5%85%B3%E4%BA%8E%E6%88%91%E4%B8%80%E5%B7%B4%E6%8E%8C%E6%8B%8D%E5%9D%8F%E7%A1%AC%E7%9B%98%E5%AF%BC%E8%87%B4%E5%8D%9A%E5%AE%A2%E6%BA%90%E4%BB%A3%E7%A0%81%E4%B8%A2%E5%A4%B1%E4%B8%80%E5%B9%B4%E5%8D%8A%E5%90%8E%E9%87%8D%E5%BB%BA%E5%8D%9A%E5%AE%A2%E8%BF%99%E4%BB%B6%E4%BA%8B/"},{"title":"使用VUE时遇到关于符号连接的一个BUG","text":"最近准备试一下ABP框架做个vue的spa应用，按着教程一步步走下去，走到运行yarn serve就报了如下错误 网上查了一下以为是某些插件没装好，不过转念想想ABP这么牛的框架不会吧不会吧不会吧。 很奇怪的是把vue复制到其他文件夹可以正常运行，唯独在source文件夹里面不能正常运行，Google了一早上后打算不用vue，最后再仔细看了一下返回的报错 才想起来source文件夹因为之前C盘空间不够移到D盘在mklink回C盘的，导致在C盘开Power shell运行D盘的文件就会报错。 解决方法在D盘开Power shell运行yarn serve即可","link":"/2020/05/10/%E4%BD%BF%E7%94%A8VUE%E6%97%B6%E9%81%87%E5%88%B0%E5%85%B3%E4%BA%8E%E7%AC%A6%E5%8F%B7%E8%BF%9E%E6%8E%A5%E7%9A%84%E4%B8%80%E4%B8%AABUG/"},{"title":"安装与配置docker","text":"系统用的是ubuntu16.04，过程参考官方文档https://docs.docker.com/engine/install/ubuntu/ 这里只记录一下坑和小技巧 别换源先使用系统自带源，别着急换国内源之前安装docker换源后执行到下面的命令会报错，提示找不到docker的安装包 1sudo apt-get install docker-ce docker-ce-cli containerd.io 这里我的解决方法是 开tsocks，使用官方源来安装docker 修改docker仓库源安装完成后直接奔着 https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors 阿里的镜像去，感觉比其他的仓库源好用 有用的命令12345678910111213141516#查看运行的容器docker ps#退出并在后台运行Ctrl + P + Q #进入容器docker attach [CONTAINER ID]#-v挂载#譬如我要启动一个centos容器，宿主机的/test目录挂载到容器的/soft目录，可通过以下方式指定：docker run -it -v /test:/soft centos /bin/bash#冒号&quot;:&quot;前面的目录是宿主机目录，后面的目录是容器内目录。#进入docker交互模式sudo docker exec -it [CONTAINER ID] /bin/bash","link":"/2020/09/08/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AEdocker/"},{"title":"记：winserver2008 的任务计划特性","text":"winserver2008 中任务计划程序运行程序时，程序的运行路径不是指定的路径下，而是 c:\\Windows\\system32\\下 解决办法 文件放到 system32 中执行。 写一个 bat 将计划中操作指向 bat 文件，bat 文件先 cd 再执行程序。","link":"/2020/02/27/%E8%AE%B0%EF%BC%9Awinserver2008-%E7%9A%84%E4%BB%BB%E5%8A%A1%E8%AE%A1%E5%88%92%E7%89%B9%E6%80%A7/"},{"title":"将堆栈星轨变成一段流星雨视频","text":"前言疫情期间一直在家呆着也没事做就拍拍星星前两天拍了张星轨 ，但是忘了关长时间降噪全是断点不怎么满意 昨天晚上又拍了一次，比较成功还误打误撞的做了一段流星雨视频（雾）打算记录一下，供以后参考 星轨的堆栈拍摄方法网上又很多教程这里就不赘述了 这里直接介绍后期处理的部分 前提： 只需要一种运动方向的星星 星星的运动轨迹是自下而上的话就需要进行反序，自上而下则不需要对图片进行反序 后期我使用的是 lr 当然也可以用 acr 但是不方便统一管理图片 将图片导入电脑后在 lr 中添加图片然后进行调整 调整好照片后我们全选刚才拍摄的照片 在照片上右键——设置——同步设置 然后进行导出，导出设置中需要尽量选择小尺寸的图片输出不然后面的处理会很卡 导出后先需要对文件进行反序排列我编写了一个程序（附件在本文最后）进行反序 将程序放到图片文件夹内双击运行后目录下会多出一个 Re 文件夹里面就是反序好的图片 接下来我们打开 pr 建立好工程文档后以图片序列的方式导入反序好的图片 添加效果中视频效果——时间——残影 设置残影数量为 80、衰减 0.95、残影运算符最大值，其他的默认 导出就可以有一段类似流星雨的视频 也算是星空的另一种表现方式吧 文件反序程序","link":"/2020/02/19/%E5%B0%86%E5%A0%86%E6%A0%88%E6%98%9F%E8%BD%A8%E5%8F%98%E6%88%90%E4%B8%80%E6%AE%B5%E6%B5%81%E6%98%9F%E9%9B%A8%E8%A7%86%E9%A2%91/"}],"tags":[{"name":".NetCore","slug":"NetCore","link":"/tags/NetCore/"},{"name":"C#","slug":"C","link":"/tags/C/"},{"name":"EFCore","slug":"EFCore","link":"/tags/EFCore/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"SQLServer","slug":"SQLServer","link":"/tags/SQLServer/"},{"name":"大数据","slug":"大数据","link":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"HBase","slug":"HBase","link":"/tags/HBase/"},{"name":"JS","slug":"JS","link":"/tags/JS/"},{"name":"动画","slug":"动画","link":"/tags/%E5%8A%A8%E7%94%BB/"},{"name":"好玩的","slug":"好玩的","link":"/tags/%E5%A5%BD%E7%8E%A9%E7%9A%84/"},{"name":"SQL","slug":"SQL","link":"/tags/SQL/"},{"name":"HQL","slug":"HQL","link":"/tags/HQL/"},{"name":"Hive","slug":"Hive","link":"/tags/Hive/"},{"name":"Mysql","slug":"Mysql","link":"/tags/Mysql/"},{"name":"mariadb","slug":"mariadb","link":"/tags/mariadb/"},{"name":"主从数据库","slug":"主从数据库","link":"/tags/%E4%B8%BB%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"备份","slug":"备份","link":"/tags/%E5%A4%87%E4%BB%BD/"},{"name":"HADOOP","slug":"HADOOP","link":"/tags/HADOOP/"},{"name":"Idea","slug":"Idea","link":"/tags/Idea/"},{"name":"Scala","slug":"Scala","link":"/tags/Scala/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"图一乐","slug":"图一乐","link":"/tags/%E5%9B%BE%E4%B8%80%E4%B9%90/"},{"name":"WINDOWS","slug":"WINDOWS","link":"/tags/WINDOWS/"},{"name":"VUE","slug":"VUE","link":"/tags/VUE/"},{"name":"符号连接","slug":"符号连接","link":"/tags/%E7%AC%A6%E5%8F%B7%E8%BF%9E%E6%8E%A5/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"服务器","slug":"服务器","link":"/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"调试","slug":"调试","link":"/tags/%E8%B0%83%E8%AF%95/"},{"name":"WINDOWS SERVER","slug":"WINDOWS-SERVER","link":"/tags/WINDOWS-SERVER/"},{"name":"好康的","slug":"好康的","link":"/tags/%E5%A5%BD%E5%BA%B7%E7%9A%84/"},{"name":"摄影","slug":"摄影","link":"/tags/%E6%91%84%E5%BD%B1/"},{"name":"照片","slug":"照片","link":"/tags/%E7%85%A7%E7%89%87/"}],"categories":[{"name":"编程","slug":"编程","link":"/categories/%E7%BC%96%E7%A8%8B/"},{"name":"数据库","slug":"编程/数据库","link":"/categories/%E7%BC%96%E7%A8%8B/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"服务器","slug":"服务器","link":"/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"网站","slug":"网站","link":"/categories/%E7%BD%91%E7%AB%99/"},{"name":"数据库","slug":"服务器/数据库","link":"/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"调试","slug":"调试","link":"/categories/%E8%B0%83%E8%AF%95/"},{"name":"摄影","slug":"摄影","link":"/categories/%E6%91%84%E5%BD%B1/"}]}